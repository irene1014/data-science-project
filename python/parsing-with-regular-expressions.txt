from nltk import pos_tag, RegexpParserfrom tokenize_words import word_sentence_tokenizefrom chunk_counters import np_chunk_counter, vp_chunk_counter# import text of choice heretext=open("dorian_gray.txt",encoding="utf-8").read().lower()# sentence and word tokenize text hereword_tokenized_text=word_sentence_tokenize(text)# store and print any word tokenized sentence heresingle_word_tokenized_sentence=word_tokenized_text[10]# create a list to hold part-of-speech tagged sentences herepos_tagged_text=list()# create a for loop through each word tokenized sentence herefor i in word_tokenized_text:  pos_tagged_text.append(pos_tag(i))single_pos_sentence=pos_tagged_text[1]  # part-of-speech tag each sentence and append to list of pos-tagged sentences here# define noun phrase chunk grammar herenp_chunk_grammar="NP: {<DT>?<JJ>*<NN>}"np_chunk_parser=RegexpParser(np_chunk_grammar)vp_chunk_grammar="VP: {<DT>?<JJ>*<NN><VP.*><RB.?>?}"vp_chunk_parser=RegexpParser(vp_chunk_grammar)np_chunked_text=list()vp_chunked_text=list()for i in pos_tagged_text:  np_chunked_text.append(np_chunk_parser.parse(i))  vp_chunked_text.append(vp_chunk_parser.parse(i))most_common_np_chunks=np_chunk_counter(np_chunked_text,30)most_common_vp_chunks=vp_chunk_counter(vp_chunked_text,30)  